{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #88ffee; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "# CMSE 830: Foundations of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Homework Assignment\n",
    "### Singular Value Decomposition & Principal Component Analysis\n",
    "\n",
    "**Student Name:** ______Brandon Webster___________  \n",
    "**Date:** ________11/16/2024________________________\n",
    "\n",
    "#### ðŸ“š Learning Objectives:\n",
    "* Understand the mathematical foundations of SVD and PCA\n",
    "* Apply dimensionality reduction techniques to real-world datasets\n",
    "* Visualize high-dimensional data in lower dimensions\n",
    "* Interpret results and explain variance explained by principal components\n",
    "\n",
    "---\n",
    "\n",
    "### A quick note on these tasks:\n",
    "* Try and answer each question *clearly*. These questions will certainly ask you to think a lot and deeply--it is perfectly reasonable to refer to ChatGPT or similar tool for help. We encourage you to use these tools, so long as you cite them using our class AI policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "## Problem 1: Lightning Questions about Linear Algebra (10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Answer these questions, using $\\LaTeX$ and markdown where needed:\n",
    "\n",
    "* What rank is the identity matrix $I$? \n",
    "    - **The rank of identity matrix is always equal to the number of columns because it is full rank**\n",
    "* If we multiply a matrix $A$ by a diagonal matrix $D$ from the left ($DA$), what does it do to the matrix? What if we multiply a matrix by a diagonal marix from the right ($AD$), what does that do? Use these two observations to give two interpretations of SVD (i.e., $U\\Sigma$ and $\\Sigma V^\\mathsf{T}$).\n",
    "    - **When multipliying from the left, each element of the i-th row of $A$ is multiplied by the i-th diagonal of $D$ which means we are scaling each row of $A$. If we multiply from the right then we are scaling each column, j, of $A$ by the j-th diagonal of $D$. In terms of SVD, $\\Sigma$, is a diagonal matrix of singular values. $U\\Sigma$ is multiplying from the right so we are scaling each column of $U$. $\\Sigma V^\\mathsf{T}$ we are thus scaling each row of V_T.**\n",
    "* If we write our data matrix $X$ in terms of its SVD and set some of the SVs (singular values) to zero, do we change the size and shape of $X$? What exactly are we doing? \n",
    "    - **When we set SVs to zero, we don't necissarily change the shape or size of X but it reduces the rank of reconstructed X. That is to say by setting the 3rd SV to zero we disregard the 3rd column of $U$ and and the 3rd row of Vt when we put X back together.**\n",
    "* When we do PCA, we need a new data matrix $X'$ that is _smaller_ - we are doing dimensionality reduction. How do we make this smaller matrix? Do we just drop columns in $X$?!\n",
    "    - **We don't necessarily drop columns but we set their contribution equal to zero by setting the corresponding SV to zero.**\n",
    "* I am often very sloppy when I say that $C = X^TX$ is the correlation matrix. In what three ways might this be misleading/incorrect? \n",
    "    - **Stating that $C = X^TX$ is the correlation matrix can be problematic because:**\n",
    "        1. a correlation matrix implies that each feature is normalized with a mean of 0 and variance of 1. Normalizing is a separate step from $C = X^TX$.\n",
    "        2. There needs to be suffecient samples in X to form a full rank, square, and symmetric matrix, $C$.\n",
    "        3. \n",
    "\n",
    "* Suppose we have a square matrix. How are its eigenvalues related to its SVs?\n",
    "    - **For a square matrix the eigenvalues are the square of singular values.**\n",
    "* Explain explained variance. Is there a connection to the SVs in some way?\n",
    "    - **Explained variance refers to the total variability in the specific dataset that is captured by that 'direction' through the data. The explained variance is captured by the singular values in SVD.**\n",
    "* What is the Kaiser criterion and how does it compare to elbow? \n",
    "    - **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "## Problem 2: Interpolation \"_By Hand_\" with the Normal Equation (10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this course you have learned three ways of doing linear regression:\n",
    "1. by hand: solving $N$ equations in $N$ unknowns for the weights; this was only practical for a very small number of weights\n",
    "2. optimization: we can cast finding the weights as the minimization of a loss function $L({\\bf w})$ that depends on the weights \n",
    "3. by inverting a matrix for the case of a square matrix (which is \"interpolation\")\n",
    "\n",
    "The third way is generalized to non-square matrices through the use of the Moore-Penrose pseudoinverse $X^+$. \n",
    "\n",
    "Begin with the Moore-Penrose pseudoinverse expression for linear regression. That is, write down the **Normal Equation**, which looks unlike the other ways we have approached linear regression! You will make them look the same here! \n",
    "\n",
    "You are given two data points $(x_1, y_1)$ and $(x_2, y_2)$ and a model $y = w_0 + w_1x$.  First write down the expressions for $w_0$ and $w_1$ in terms of the data, using $\\LaTeX$. You have done this before, so don't spend too much time on this - go find it! I want you to have the equations here in front of you. \n",
    "\n",
    "Next show **all** of the math steps to derive the weights from the **Normal Equation**. That is, in the **Normal Equation** write down what ${\\bf y}$ is, what $X$ is, what $X^TX$ is and so on. Do all steps by hand in $\\LaTeX$. One you have the vector ${\\bf w}$ separate it into $w_0$ and $w_1$ and compare to what you got with the more direct way. \n",
    "\n",
    "Recall that I showed in class that the inverse of a $2\\times 2$ matrix can be done analytically and I gave the expression in the slides.\n",
    "\n",
    "Does the **Normal Equation** yield the same expressions for $w_0$ and $w_1$ as you obtained by solving two equations in two unknowns? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "## Problem 3: PCA Understanding (10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Answer these questions in a markdown cell using equations where necessary:\n",
    "1. When we use SVD, we use three matrices $U$, $\\Sigma$ and $V$; when we work with the covariance matrix $C$, we often focus on $V$ - why is that? \n",
    "2. What are the eigenvectors of $C$? What are the eigenvalues?\n",
    "3. What purpose does $U$ have? Do $V$ and $U$ have different purposes? If so, why? If not, why not? What purpose does $\\Sigma$ have?\n",
    "4. In what ways does the use of $C$, rather than just $X$ itself, inform us to the meanings of $U$ and $V$? \n",
    "5. Given PCA's focus on $C$, what does PCA reveal to us geometrically? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "## Problem 4: PCA Visualization (10)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Part 1: Data Preparation and SVD Basics\n",
    "1. Load the Palmer Penguins dataset and prepare it:\n",
    "   - Handle missing values\n",
    "   - Select only numeric columns\n",
    "   - Scale the data appropriately\n",
    "   - Keep track of species for later coloring (next problem)\n",
    "\n",
    "2. Perform SVD step by step using `linalg`:\n",
    "   - Compute $U$, $\\Sigma$, and $V^\\mathsf{T}$ matrices\n",
    "   - Visualize each matrix using heatmaps (be careful with the shape of $\\Sigma$ that is returned; you will need the $\\sigma_i$ and $\\lambda_i$ below)\n",
    "   - Explain what each matrix represents in terms of the penguins data\n",
    "\n",
    "#### Part 2: Understanding Variance\n",
    "1. Create and interpret scree plots:\n",
    "   - Plot singular values\n",
    "   - Calculate and plot explained variance ratios\n",
    "   - Create cumulative variance plot\n",
    "   - Identify \"elbow\" point\n",
    "   - Justify how many components you would keep\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f5e9; padding: 20px; border-radius: 10px; margin: 10px 0; max-width: 100%;\">\n",
    "\n",
    "## Problem 5: EDA/Visualization with PCA (10)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using your Penguin dataset, perform the following analyses using only `linalg` and no canned libaries:\n",
    "\n",
    "1. **Principal Component Selection**\n",
    "   - Calculate eigenvalues for all components\n",
    "   - Create a scree plot showing:\n",
    "     * Individual variance explained\n",
    "     * Cumulative variance explained\n",
    "   - Apply Kaiser criterion ($\\lambda > 1$); that is, add a horizontal line at the apprpriat height\n",
    "   - Justify your choice of number of components (markdown)\n",
    "\n",
    "2. **Score Plot Analysis**\n",
    "   - Create PC1 vs PC2 scatter plot\n",
    "   - Color points by class/category\n",
    "   - Add 95% confidence ellipses for each class\n",
    "   - Label axes with variance explained\n",
    "   - Interpret the clustering patterns\n",
    "\n",
    "3. **Loading Plot Analysis**\n",
    "   - Plot feature loadings as vectors\n",
    "   - Scale arrows by loading magnitude\n",
    "   - Label features clearly\n",
    "   - Identify which features contribute most to each PC (markdown)\n",
    "\n",
    "4. **Biplot Integration**\n",
    "   - Combine scores and loadings in one plot\n",
    "   - Ensure proper scaling of both components\n",
    "   - Add a clear legend distinguishing:\n",
    "     * Observations (points)\n",
    "     * Features (arrows)\n",
    "     * Class boundaries (ellipses)\n",
    "\n",
    "5. **Interpretation & Insights**\n",
    "   - Explain relationships between:\n",
    "     * Features (using loading angles)\n",
    "     * Classes (using score positions)\n",
    "     * Features and classes (using biplot)\n",
    "   - Identify which features best distinguish classes\n",
    "   - Note any surprising patterns or outliers (markdown)\n",
    "\n",
    "**Deliverables:**\n",
    "- All plots with clear labels and legends\n",
    "- Brief explanations of your findings (markdown)\n",
    "- Justification for component selection (markdown)\n",
    "- Discussion of practical implications (markdown)\n",
    "\n",
    "**Tips:**\n",
    "- Standardize your data before PCA\n",
    "- Use consistent colors across all plots\n",
    "- Add grid lines for easier interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
